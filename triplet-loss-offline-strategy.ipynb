{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"##### This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-25T04:32:47.048473Z","iopub.execute_input":"2023-04-25T04:32:47.049069Z","iopub.status.idle":"2023-04-25T04:32:47.076232Z","shell.execute_reply.started":"2023-04-25T04:32:47.048991Z","shell.execute_reply":"2023-04-25T04:32:47.074335Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/matrix-lsa/LSA64_matrix.csv\n/kaggle/input/matrix-lsamediapipe/padded_matrix_file (1).csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport csv\nimport ast\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.manifold import TSNE\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2023-04-25T04:32:47.079314Z","iopub.execute_input":"2023-04-25T04:32:47.080640Z","iopub.status.idle":"2023-04-25T04:32:49.100340Z","shell.execute_reply.started":"2023-04-25T04:32:47.080575Z","shell.execute_reply":"2023-04-25T04:32:49.098837Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Your code for loading the data (unchanged)\ninput_file = \"/kaggle/input/matrix-lsamediapipe/padded_matrix_file (1).csv\"\n\nvalues = []\nmatrix_labels = []\nnum_rows = 0\n\nwith open(input_file, \"r\") as f_input:\n    reader = csv.reader(f_input)\n    for row in reader:\n        row_values = []\n        for i in range(len(row) - 1):\n            column_value = ast.literal_eval(row[i])\n            row_values.append(column_value)\n        values.append(torch.tensor(row_values))\n        matrix_labels.append(ast.literal_eval(row[-1]))\n        num_rows += 1\n\n# matrix_labels = [label - 1 for label in matrix_labels]","metadata":{"execution":{"iopub.status.busy":"2023-04-25T04:32:49.102217Z","iopub.execute_input":"2023-04-25T04:32:49.103089Z","iopub.status.idle":"2023-04-25T04:36:02.384431Z","shell.execute_reply.started":"2023-04-25T04:32:49.102996Z","shell.execute_reply":"2023-04-25T04:36:02.382933Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def sample_triplets(data, labels):\n    anchor_indices = torch.arange(len(data))\n    positive_indices = torch.empty(len(data), dtype=torch.long)\n    negative_indices = torch.empty(len(data), dtype=torch.long)\n\n    for i, label in enumerate(labels):\n        # Sample positive instances\n        positive_candidates = (labels == label).nonzero(as_tuple=False).view(-1)\n        positive_candidates = positive_candidates[positive_candidates != i]\n        \n        if len(positive_candidates) > 0:\n            positive_indices[i] = positive_candidates[torch.randint(0, len(positive_candidates), (1,))]\n        else:\n            positive_indices[i] = i  # Set the positive index as itself if there are no other instances with the same label\n        # Sample negative instances\n        negative_candidates = (labels != label).nonzero(as_tuple=False).view(-1)\n        negative_indices[i] = negative_candidates[torch.randint(0, len(negative_candidates), (1,))]\n        \n    anchor = data[anchor_indices]\n    positive = data[positive_indices]\n    negative = data[negative_indices]\n    return anchor, positive, negative","metadata":{"execution":{"iopub.status.busy":"2023-04-25T04:36:02.387190Z","iopub.execute_input":"2023-04-25T04:36:02.387583Z","iopub.status.idle":"2023-04-25T04:36:02.399459Z","shell.execute_reply.started":"2023-04-25T04:36:02.387546Z","shell.execute_reply":"2023-04-25T04:36:02.397766Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Triplet loss function\nclass TripletLoss(nn.Module):\n    def __init__(self, margin=1.0):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, anchor, positive, negative):\n        pos_distance = F.pairwise_distance(anchor,positive)\n        neg_distance = F.pairwise_distance(anchor, negative)\n        loss = F.relu(pos_distance - neg_distance + self.margin)\n        return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2023-04-25T04:36:02.401354Z","iopub.execute_input":"2023-04-25T04:36:02.402104Z","iopub.status.idle":"2023-04-25T04:36:02.419379Z","shell.execute_reply.started":"2023-04-25T04:36:02.402059Z","shell.execute_reply":"2023-04-25T04:36:02.417170Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Define the dataset class\nclass SkeletonDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]","metadata":{"execution":{"iopub.status.busy":"2023-04-25T04:36:02.421505Z","iopub.execute_input":"2023-04-25T04:36:02.423119Z","iopub.status.idle":"2023-04-25T04:36:02.433370Z","shell.execute_reply.started":"2023-04-25T04:36:02.423018Z","shell.execute_reply":"2023-04-25T04:36:02.431415Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoder(nn.Module):\n    def __init__(self, n_features, d_model=64, nhead=16, num_layers=2):\n        super(TransformerEncoder, self).__init__()\n        self.embedding = nn.Linear(n_features, d_model)\n        self.positional_encoding = self.generate_positional_encoding(d_model)\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, nhead), num_layers\n        )\n\n    def generate_positional_encoding(self, d_model, max_len=243):\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        return pe.unsqueeze(0)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = x + self.positional_encoding[:, : x.size(1)]\n        x = self.transformer_encoder(x)\n        x = x.mean(dim=1)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-04-25T04:36:02.435238Z","iopub.execute_input":"2023-04-25T04:36:02.435690Z","iopub.status.idle":"2023-04-25T04:36:02.449258Z","shell.execute_reply.started":"2023-04-25T04:36:02.435606Z","shell.execute_reply":"2023-04-25T04:36:02.447924Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into train, validation, and test sets\ntrain_data, test_data, train_labels, test_labels = train_test_split(\n    torch.stack(values), torch.tensor(matrix_labels), test_size=0.4, random_state=42, stratify=torch.tensor(matrix_labels)\n)\n\nvalidation_data, test_data, validation_labels, test_labels = train_test_split(\n    test_data, test_labels, test_size=0.5, random_state=42, stratify=test_labels\n)\n\n# Create train, validation, and test datasets\ntrain_dataset = SkeletonDataset(train_data, train_labels)\nvalidation_dataset = SkeletonDataset(validation_data, validation_labels)\ntest_dataset = SkeletonDataset(test_data, test_labels)\n\n# Create data loaders for train, validation, and test sets\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalidation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nmodel = TransformerEncoder(114)\ncriterion = TripletLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntrain_losses = []\nvalidation_losses = []\n\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_train_loss = 0\n    for batch_idx, (data, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        anchor, positive, negative = sample_triplets(data, labels)\n        anchor_embeddings = model(anchor)\n        positive_embeddings = model(positive)\n        negative_embeddings = model(negative)\n        loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n        loss.backward()\n        optimizer.step()\n        epoch_train_loss += loss.item()\n\n    epoch_train_loss /= len(train_loader)\n    train_losses.append(epoch_train_loss)\n\n    # Calculate validation loss\n    model.eval()\n    epoch_validation_loss = 0\n    with torch.no_grad():\n        for batch_idx, (data, labels) in enumerate(validation_loader):\n            anchor, positive, negative = sample_triplets(data, labels)\n            anchor_embeddings = model(anchor)\n            positive_embeddings = model(positive)\n            negative_embeddings = model(negative)\n            loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n            epoch_validation_loss += loss.item()\n\n    epoch_validation_loss /= len(validation_loader)\n    validation_losses.append(epoch_validation_loss)\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_validation_loss:.4f}\")\n\n    # Save the model\n    torch.save(model.state_dict(), f\"model_epoch_{epoch + 1}.pt\")\n\n# Plot training and validation losses\nplt.plot(train_losses, label=\"Training Loss\")\nplt.plot(validation_losses, label=\"Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-25T04:36:02.451311Z","iopub.execute_input":"2023-04-25T04:36:02.452095Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 0.2887, Validation Loss: 0.2088\nEpoch 2/100, Train Loss: 0.2000, Validation Loss: 0.1632\nEpoch 3/100, Train Loss: 0.2056, Validation Loss: 0.1527\nEpoch 4/100, Train Loss: 0.1825, Validation Loss: 0.1505\nEpoch 5/100, Train Loss: 0.1714, Validation Loss: 0.1309\nEpoch 6/100, Train Loss: 0.1732, Validation Loss: 0.1117\nEpoch 7/100, Train Loss: 0.1321, Validation Loss: 0.1118\nEpoch 8/100, Train Loss: 0.1476, Validation Loss: 0.1401\nEpoch 9/100, Train Loss: 0.1310, Validation Loss: 0.1321\n","output_type":"stream"}]},{"cell_type":"code","source":"# Extract embeddings for the test set\nmodel.eval()\ntest_embeddings = []\ntest_labels_list = []\n\nwith torch.no_grad():\n    for data, labels in train_loader:\n        embeddings = model(data)\n        test_embeddings.append(embeddings)\n        test_labels_list.append(labels)\n\ntest_embeddings = torch.cat(test_embeddings).cpu().numpy()\ntest_labels_list = torch.cat(test_labels_list).cpu().numpy()\n\n# Reduce dimensionality of the embeddings using t-SNE\ntsne = TSNE(n_components=3, random_state=42)\ntest_embeddings_3d = tsne.fit_transform(test_embeddings)\n\n# Create a label encoder to convert class labels to integers\nle = LabelEncoder()\ntest_labels_int = le.fit_transform(test_labels_list)\n\n# Plot the 3D scatter plot with multiple subplots using plotly\nn_classes = 64\nn_subplots = 10\nn_classes_per_subplot = n_classes // n_subplots\n\nsubplot_titles = [\n    f\"Embeddings for classes {i * n_classes_per_subplot}-{(i + 1) * n_classes_per_subplot - 1}\"\n    for i in range(n_subplots)\n]\nfig = make_subplots(rows=n_subplots // 2, cols=2, specs=[[{'type': 'scatter3d'}] * 2] * (n_subplots // 2), subplot_titles=subplot_titles)\n\nfor i in range(n_subplots):\n    class_indices = np.arange(i * n_classes_per_subplot, (i + 1) * n_classes_per_subplot)\n\n    for idx in class_indices:\n        class_mask = test_labels_int == idx\n        fig.add_trace(\n            go.Scatter3d(\n                x=test_embeddings_3d[class_mask, 0],\n                y=test_embeddings_3d[class_mask, 1],\n                z=test_embeddings_3d[class_mask, 2],\n                mode=\"markers\",\n                name=str(le.inverse_transform([idx])[0]),\n                marker=dict(size=3),\n                showlegend=False,\n            ),\n            row=i // 2 + 1,\n            col=i % 2 + 1,\n        )\n\nfig.update_layout(height=2000, width=1000, title_text=\"Embeddings Visualizations\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}