{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-29T07:13:47.138850Z","iopub.execute_input":"2023-04-29T07:13:47.139646Z","iopub.status.idle":"2023-04-29T07:13:47.180151Z","shell.execute_reply.started":"2023-04-29T07:13:47.139589Z","shell.execute_reply":"2023-04-29T07:13:47.178947Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/lsa64-matrix-10-classes-v1/padded_matrix_file.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport csv\nimport ast\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.manifold import TSNE\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2023-04-29T07:13:47.181901Z","iopub.execute_input":"2023-04-29T07:13:47.182471Z","iopub.status.idle":"2023-04-29T07:13:51.621901Z","shell.execute_reply.started":"2023-04-29T07:13:47.182434Z","shell.execute_reply":"2023-04-29T07:13:51.620588Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Your code for loading the data (unchanged)\ninput_file = \"/kaggle/input/lsa64-matrix-10-classes-v1/padded_matrix_file.csv\"\n\nvalues = []\nmatrix_labels = []\nnum_rows = 0\n\nwith open(input_file, \"r\") as f_input:\n    reader = csv.reader(f_input)\n    for row in reader:\n        row_values = []\n        for i in range(len(row) - 1):\n            column_value = ast.literal_eval(row[i])\n            row_values.append(column_value)\n        values.append(torch.tensor(row_values))\n        matrix_labels.append(ast.literal_eval(row[-1]))\n        num_rows += 1","metadata":{"execution":{"iopub.status.busy":"2023-04-29T07:13:51.623563Z","iopub.execute_input":"2023-04-29T07:13:51.624252Z","iopub.status.idle":"2023-04-29T07:14:21.121223Z","shell.execute_reply.started":"2023-04-29T07:13:51.624199Z","shell.execute_reply":"2023-04-29T07:14:21.119945Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def sample_triplets(data, labels, model, margin):\n    anchor_indices = torch.arange(len(data))\n    positive_indices = torch.empty(len(data), dtype=torch.long)\n    negative_indices = torch.empty(len(data), dtype=torch.long)\n\n    with torch.no_grad():\n        embeddings = model(data)\n\n    for i, label in enumerate(labels):\n        # Sample positive instances\n        positive_candidates = (labels == label).nonzero(as_tuple=False).view(-1)\n        positive_candidates = positive_candidates[positive_candidates != i]\n\n        if len(positive_candidates) > 0:\n            positive_indices[i] = positive_candidates[torch.randint(0, len(positive_candidates), (1,))]\n        else:\n            positive_indices[i] = i\n\n        # Sample negative instances\n        negative_candidates = (labels != label).nonzero(as_tuple=False).view(-1)\n        anchor_embedding = embeddings[i]\n        negative_embeddings = embeddings[negative_candidates]\n        pos_distance = torch.norm(anchor_embedding - embeddings[positive_indices[i]])\n\n        neg_distances = torch.norm(anchor_embedding.unsqueeze(0) - negative_embeddings, dim=1)\n        semi_hard_negatives = (neg_distances > (pos_distance - margin)) & (neg_distances < (pos_distance + margin))\n\n        if torch.any(semi_hard_negatives):\n            negative_indices[i] = negative_candidates[semi_hard_negatives][torch.randint(0, semi_hard_negatives.sum(), (1,))]\n        else:\n            negative_indices[i] = negative_candidates[neg_distances.argmin()]\n\n    anchor = data[anchor_indices]\n    positive = data[positive_indices]\n    negative = data[negative_indices]\n    return anchor, positive, negative\n","metadata":{"execution":{"iopub.status.busy":"2023-04-29T07:14:21.123598Z","iopub.execute_input":"2023-04-29T07:14:21.124288Z","iopub.status.idle":"2023-04-29T07:14:21.136775Z","shell.execute_reply.started":"2023-04-29T07:14:21.124250Z","shell.execute_reply":"2023-04-29T07:14:21.135628Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Triplet loss function\nclass TripletLoss(nn.Module):\n    def __init__(self, margin=1.0):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, anchor, positive, negative):\n        pos_distance = F.pairwise_distance(anchor,positive)\n        neg_distance = F.pairwise_distance(anchor, negative)\n        loss = F.relu(pos_distance - neg_distance + self.margin)\n        return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T07:14:21.138458Z","iopub.execute_input":"2023-04-29T07:14:21.138876Z","iopub.status.idle":"2023-04-29T07:14:21.157686Z","shell.execute_reply.started":"2023-04-29T07:14:21.138840Z","shell.execute_reply":"2023-04-29T07:14:21.156381Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Define the dataset class\nclass SkeletonDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]","metadata":{"execution":{"iopub.status.busy":"2023-04-29T07:14:21.159693Z","iopub.execute_input":"2023-04-29T07:14:21.160271Z","iopub.status.idle":"2023-04-29T07:14:21.173626Z","shell.execute_reply.started":"2023-04-29T07:14:21.160216Z","shell.execute_reply":"2023-04-29T07:14:21.172668Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoder(nn.Module):\n    def __init__(self, n_features, d_model=64, nhead=64, num_layers=1):\n        super(TransformerEncoder, self).__init__()\n        self.embedding = nn.Linear(n_features, d_model)\n        self.positional_encoding = self.generate_positional_encoding(d_model)\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, nhead), num_layers\n        )\n\n    def generate_positional_encoding(self, d_model, max_len=243):\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        return pe.unsqueeze(0)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = x + self.positional_encoding[:, : x.size(1)]\n        x = self.transformer_encoder(x)\n        x = x.mean(dim=1)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-04-29T07:14:21.175195Z","iopub.execute_input":"2023-04-29T07:14:21.175523Z","iopub.status.idle":"2023-04-29T07:14:21.186320Z","shell.execute_reply.started":"2023-04-29T07:14:21.175491Z","shell.execute_reply":"2023-04-29T07:14:21.185056Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into train, validation, and test sets\ntrain_data, test_data, train_labels, test_labels = train_test_split(\n    torch.stack(values), torch.tensor(matrix_labels), test_size=0.4, random_state=42, stratify=torch.tensor(matrix_labels)\n)\n\nvalidation_data, test_data, validation_labels, test_labels = train_test_split(\n    test_data, test_labels, test_size=0.5, random_state=42, stratify=test_labels\n)\n\n# Create train, validation, and test datasets\ntrain_dataset = SkeletonDataset(train_data, train_labels)\nvalidation_dataset = SkeletonDataset(validation_data, validation_labels)\ntest_dataset = SkeletonDataset(test_data, test_labels)\n\n# Create data loaders for train, validation, and test sets\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalidation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nmodel = TransformerEncoder(114)\ncriterion = TripletLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntrain_losses = []\nvalidation_losses = []\n\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_train_loss = 0\n    for batch_idx, (data, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        anchor, positive, negative = sample_triplets(data, labels, model, margin=1.0)\n        anchor_embeddings = model(anchor)\n        positive_embeddings = model(positive)\n        negative_embeddings = model(negative)\n        loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n        loss.backward()\n        optimizer.step()\n        epoch_train_loss += loss.item()\n\n    epoch_train_loss /= len(train_loader)\n    train_losses.append(epoch_train_loss)\n\n    # Calculate validation loss\n    model.eval()\n    epoch_validation_loss = 0\n    with torch.no_grad():\n        for batch_idx, (data, labels) in enumerate(validation_loader):\n            anchor, positive, negative = sample_triplets(data, labels, model, margin=1.0)\n            anchor_embeddings = model(anchor)\n            positive_embeddings = model(positive)\n            negative_embeddings = model(negative)\n            loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n            epoch_validation_loss += loss.item()\n\n    epoch_validation_loss /= len(validation_loader)\n    validation_losses.append(epoch_validation_loss)\n\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_validation_loss:.4f}\")\n\n    # Save the model\n    torch.save(model.state_dict(), f\"model_epoch_{epoch + 1}.pt\")\n\n# Plot training and validation losses\nplt.plot(train_losses, label=\"Training Loss\")\nplt.plot(validation_losses, label=\"Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T07:26:27.240652Z","iopub.execute_input":"2023-04-29T07:26:27.241081Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50, Train Loss: 0.7259, Validation Loss: 0.5011\nEpoch 2/50, Train Loss: 0.6166, Validation Loss: 0.4804\nEpoch 3/50, Train Loss: 0.6161, Validation Loss: 0.6932\n","output_type":"stream"}]},{"cell_type":"code","source":"# Extract embeddings for the test set\nmodel.eval()\ntest_embeddings = []\ntest_labels_list = []\n\nwith torch.no_grad():\n    for data, labels in train_loader:\n        embeddings = model(data)\n        test_embeddings.append(embeddings)\n        test_labels_list.append(labels)\n\ntest_embeddings = torch.cat(test_embeddings).cpu().numpy()\ntest_labels_list = torch.cat(test_labels_list).cpu().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2D visualizations\ntsne = TSNE(n_components=2, random_state=42)  # changed n_components to 2\ntest_embeddings_2d = tsne.fit_transform(test_embeddings)  # changed variable name to test_embeddings_2d\n\nle = LabelEncoder()\ntest_labels_int = le.fit_transform(test_labels_list)\n\nn_classes = 10\nn_subplots = 2\nn_classes_per_subplot = n_classes // n_subplots\n\nsubplot_titles = [\n  f\"Embeddings for classes {i * n_classes_per_subplot}-{(i + 1) * n_classes_per_subplot - 1}\"\n  for i in range(n_subplots)\n]\nfig = make_subplots(rows=n_subplots // 2, cols=2, specs=[[{'type': 'scatter'}] * 2] * (n_subplots // 2), subplot_titles=subplot_titles)\n\nfor i in range(n_subplots):\n  class_indices = np.arange(i * n_classes_per_subplot, (i + 1) * n_classes_per_subplot)\n\n  for idx in class_indices:\n      class_mask = test_labels_int == idx\n      fig.add_trace(\n          go.Scatter(\n              x=test_embeddings_2d[class_mask, 0],  # changed to use test_embeddings_2d\n              y=test_embeddings_2d[class_mask, 1],  # changed to use test_embeddings_2d\n              mode=\"markers\",\n              name=str(le.inverse_transform([idx])[0]),\n              marker=dict(size=3),\n              showlegend=False,\n          ),\n          row=i // 2 + 1,\n          col=i % 2 + 1,\n      )\n\nfig.update_layout(height=400, width=600, title_text=\"Embeddings Visualizations (2D)\")  # changed height to 1000\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}