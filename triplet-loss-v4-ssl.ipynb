{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-23T06:54:03.246243Z","iopub.execute_input":"2023-04-23T06:54:03.247687Z","iopub.status.idle":"2023-04-23T06:54:03.274282Z","shell.execute_reply.started":"2023-04-23T06:54:03.247625Z","shell.execute_reply":"2023-04-23T06:54:03.273095Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/input/ssl-matrix-file/padded_matrix_file.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"def sample_triplets(data, labels):\n    anchor_indices = torch.arange(len(data))\n    positive_indices = torch.empty(len(data), dtype=torch.long)\n    negative_indices = torch.empty(len(data), dtype=torch.long)\n\n    for i, label in enumerate(labels):\n        # Sample positive instances\n        positive_candidates = (labels == label).nonzero(as_tuple=False).view(-1)\n        positive_candidates = positive_candidates[positive_candidates != i]\n        \n        if len(positive_candidates) > 0:\n            positive_indices[i] = positive_candidates[torch.randint(0, len(positive_candidates), (1,))]\n        else:\n            positive_indices[i] = i  # Set the positive index as itself if there are no other instances with the same label\n#         print('positive',positive_indices)\n        # Sample negative instances\n        negative_candidates = (labels != label).nonzero(as_tuple=False).view(-1)\n        negative_indices[i] = negative_candidates[torch.randint(0, len(negative_candidates), (1,))]\n#         print('positive',negative_indices)\n        \n    anchor = data[anchor_indices]\n    positive = data[positive_indices]\n    negative = data[negative_indices]\n    return anchor, positive, negative\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T06:54:03.276207Z","iopub.execute_input":"2023-04-23T06:54:03.276593Z","iopub.status.idle":"2023-04-23T06:54:03.287232Z","shell.execute_reply.started":"2023-04-23T06:54:03.276555Z","shell.execute_reply":"2023-04-23T06:54:03.285830Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport csv\nimport ast\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2023-04-23T06:54:03.289314Z","iopub.execute_input":"2023-04-23T06:54:03.290480Z","iopub.status.idle":"2023-04-23T06:54:03.305000Z","shell.execute_reply.started":"2023-04-23T06:54:03.290419Z","shell.execute_reply":"2023-04-23T06:54:03.303808Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Your code for loading the data (unchanged)\ninput_file = \"/kaggle/input/ssl-matrix-file/padded_matrix_file.csv\"\n\nvalues = []\nmatrix_labels = []\nnum_rows = 0\n\nwith open(input_file, \"r\") as f_input:\n    reader = csv.reader(f_input)\n    for row in reader:\n        row_values = []\n        for i in range(len(row) - 1):\n            column_value = ast.literal_eval(row[i])\n            row_values.append(column_value)\n        values.append(torch.tensor(row_values))\n        matrix_labels.append(ast.literal_eval(row[-1]))\n        num_rows += 1\n\n# matrix_labels = [label - 1 for label in matrix_labels]","metadata":{"execution":{"iopub.status.busy":"2023-04-23T06:54:03.306186Z","iopub.execute_input":"2023-04-23T06:54:03.307302Z","iopub.status.idle":"2023-04-23T06:54:32.680522Z","shell.execute_reply.started":"2023-04-23T06:54:03.307262Z","shell.execute_reply":"2023-04-23T06:54:32.679337Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Define the dataset class\nclass SkeletonDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]","metadata":{"execution":{"iopub.status.busy":"2023-04-23T06:54:32.683210Z","iopub.execute_input":"2023-04-23T06:54:32.683729Z","iopub.status.idle":"2023-04-23T06:54:32.690091Z","shell.execute_reply.started":"2023-04-23T06:54:32.683686Z","shell.execute_reply":"2023-04-23T06:54:32.688817Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Define the transformer model with positional encoding without the classifier layer\nclass TransformerEncoder(nn.Module):\n    def __init__(self, n_features, d_model=64, nhead=16, num_layers=2):\n        super(TransformerEncoder, self).__init__()\n        self.embedding = nn.Linear(n_features, d_model)\n        self.positional_encoding = self.generate_positional_encoding(d_model)\n        self.transformer_encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model, nhead), num_layers\n        )\n\n    def generate_positional_encoding(self, d_model, max_len=204):\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        return pe.unsqueeze(0)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = x + self.positional_encoding[:, : x.size(1)]\n        x = self.transformer_encoder(x)\n        x = x.mean(dim=1)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T06:54:32.691661Z","iopub.execute_input":"2023-04-23T06:54:32.692832Z","iopub.status.idle":"2023-04-23T06:54:32.705013Z","shell.execute_reply.started":"2023-04-23T06:54:32.692785Z","shell.execute_reply":"2023-04-23T06:54:32.703786Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Triplet loss function\nclass TripletLoss(nn.Module):\n    def __init__(self, margin=1.0):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, anchor, positive, negative):\n        pos_distance = F.pairwise_distance(anchor,positive)\n        neg_distance = F.pairwise_distance(anchor, negative)\n        loss = F.relu(pos_distance - neg_distance + self.margin)\n        return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2023-04-23T06:54:32.706315Z","iopub.execute_input":"2023-04-23T06:54:32.706810Z","iopub.status.idle":"2023-04-23T06:54:32.721761Z","shell.execute_reply.started":"2023-04-23T06:54:32.706760Z","shell.execute_reply":"2023-04-23T06:54:32.720540Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Split the dataset into train and test sets\nfrom sklearn.model_selection import train_test_split\n\n# Split the dataset into train and test sets\ntrain_data, test_data, train_labels, test_labels = train_test_split(\n    torch.stack(values), torch.tensor(matrix_labels), test_size=0.2, random_state=42, stratify=torch.tensor(matrix_labels)\n)\n\n# Create train and test datasets\ntrain_dataset = SkeletonDataset(train_data, train_labels)\ntest_dataset = SkeletonDataset(test_data, test_labels)\n\n# Create data loaders for train and test sets\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nmodel = TransformerEncoder(114)\ncriterion = TripletLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n    for batch_idx, (data, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        # Sample triplets from the data\n        anchor, positive, negative = sample_triplets(data, labels)\n        # Get embeddings for anchor, positive, and negative instances\n        anchor_embeddings = model(anchor)\n        positive_embeddings = model(positive)\n        negative_embeddings = model(negative)\n        loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    epoch_loss /= len(train_loader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-23T06:55:47.869478Z","iopub.execute_input":"2023-04-23T06:55:47.869978Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/20, Loss: 0.4045\nEpoch 2/20, Loss: 0.4018\nEpoch 3/20, Loss: 0.3056\nEpoch 4/20, Loss: 0.2913\nEpoch 5/20, Loss: 0.2383\nEpoch 6/20, Loss: 0.1339\nEpoch 7/20, Loss: 0.0946\nEpoch 8/20, Loss: 0.2481\nEpoch 9/20, Loss: 0.1709\nEpoch 10/20, Loss: 0.1969\nEpoch 11/20, Loss: 0.2122\nEpoch 12/20, Loss: 0.2710\nEpoch 13/20, Loss: 0.1091\nEpoch 14/20, Loss: 0.1606\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Compute embeddings for train and test data\ntrain_embeddings = []\ntrain_labels_list = []\n\nmodel.eval()\nwith torch.no_grad():\n    for data, labels in train_loader:\n        embeddings = model(data)\n        train_embeddings.append(embeddings)\n        train_labels_list.append(labels)\n\ntrain_embeddings = torch.cat(train_embeddings).numpy()\ntrain_labels_list = torch.cat(train_labels_list).numpy()\n\ntest_embeddings = []\ntest_labels_list = []\n\nwith torch.no_grad():\n    for data, labels in test_loader:\n        embeddings = model(data)\n        test_embeddings.append(embeddings)\n        test_labels_list.append(labels)\n\ntest_embeddings = torch.cat(test_embeddings).numpy()\ntest_labels_list = torch.cat(test_labels_list).numpy()\n\n# Train a k-NN classifier on the train embeddings and evaluate on test embeddings\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(train_embeddings, train_labels_list)\n\ntest_predictions = knn.predict(test_embeddings)\naccuracy = accuracy_score(test_labels_list, test_predictions)\n\nprint(num_epochs)\nprint(\"Accuracy:\", accuracy)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_embeddings[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-23T06:54:32.908454Z","iopub.status.idle":"2023-04-23T06:54:32.908928Z","shell.execute_reply.started":"2023-04-23T06:54:32.908714Z","shell.execute_reply":"2023-04-23T06:54:32.908740Z"},"trusted":true},"execution_count":null,"outputs":[]}]}